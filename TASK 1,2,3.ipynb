{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f21b423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/kemboi/anaconda3/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/kemboi/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82986d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8625ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.read_csv(\"data.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f98447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "221bd154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/01 19:53:50 WARN Utils: Your hostname, Kemboi resolves to a loopback address: 127.0.1.1; using 10.3.3.191 instead (on interface wlo1)\n",
      "23/03/01 19:53:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/01 19:53:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName(\"Task2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c31e4ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.3.3.191:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Task2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f83d818adc0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc904fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da0f43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read.option(\"header\",\"true\").csv(\"data.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e3ae5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/01 19:56:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "# Compute summary statistics\n",
    "summary = df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57866838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  summary                 _c0                _c1                  _c2  \\\n",
      "0   count             1048576            1048576              1048576   \n",
      "1    mean   4876.261774312758  8.107557399327659                 None   \n",
      "2  stddev  14443.436003519171   4.46062464438205                 None   \n",
      "3     min                   0                  0  05/01/1970 03:01:17   \n",
      "4     max            Dst Port           Protocol            Timestamp   \n",
      "\n",
      "                    _c3                 _c4                 _c5  \\\n",
      "0               1048576             1048576             1048576   \n",
      "1     6255554.608775719   6.206622320768663   7.211191378775958   \n",
      "2  1.2602911086328785E9  44.478512313301124  104.86818042875132   \n",
      "3          -11873000000                   1                   0   \n",
      "4         Flow Duration        Tot Fwd Pkts        Tot Bwd Pkts   \n",
      "\n",
      "                  _c6                _c7                 _c8  ...  \\\n",
      "0             1048576            1048576             1048576  ...   \n",
      "1  447.99364280094414  4521.803392222779  174.57359273299477  ...   \n",
      "2  15735.414790849307  151502.1211764956   287.6713000641285  ...   \n",
      "3                   0                  0                   0  ...   \n",
      "4     TotLen Fwd Pkts    TotLen Bwd Pkts     Fwd Pkt Len Max  ...   \n",
      "\n",
      "                 _c70                _c71                _c72  \\\n",
      "0             1048576             1048576             1048576   \n",
      "1  23.279704360679972  51524.487969143585   21361.50958699804   \n",
      "2  11.061852084114566   581558.6342225706  218640.49732394647   \n",
      "3                   0                   0                   0   \n",
      "4    Fwd Seg Size Min         Active Mean          Active Std   \n",
      "\n",
      "                _c73                _c74                 _c75  \\\n",
      "0            1048576             1048576              1048576   \n",
      "1  87891.57214410033  39954.774385237106    3101206.320220606   \n",
      "2  739572.4666968074   560269.2675063498  5.414779701174505E8   \n",
      "3                  0                   0                    0   \n",
      "4         Active Max          Active Min            Idle Mean   \n",
      "\n",
      "                   _c76                  _c77                  _c78  \\\n",
      "0               1048576               1048576               1048576   \n",
      "1     729721.7626003549     4812390.631669647    2126919.6016422287   \n",
      "2  3.8200308516858596E8  1.5221170424266853E9  1.8170133217088945E7   \n",
      "3                     0                     0                     0   \n",
      "4              Idle Std              Idle Max              Idle Min   \n",
      "\n",
      "             _c79  \n",
      "0         1048576  \n",
      "1            None  \n",
      "2            None  \n",
      "3          Benign  \n",
      "4  SSH-Bruteforce  \n",
      "\n",
      "[5 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print summary statistics\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93875a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', '_c13', '_c14', '_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22', '_c23', '_c24', '_c25', '_c26', '_c27', '_c28', '_c29', '_c30', '_c31', '_c32', '_c33', '_c34', '_c35', '_c36', '_c37', '_c38', '_c39', '_c40', '_c41', '_c42', '_c43', '_c44', '_c45', '_c46', '_c47', '_c48', '_c49', '_c50', '_c51', '_c52', '_c53', '_c54', '_c55', '_c56', '_c57', '_c58', '_c59', '_c60', '_c61', '_c62', '_c63', '_c64', '_c65', '_c66', '_c67', '_c68', '_c69', '_c70', '_c71', '_c72', '_c73', '_c74', '_c75', '_c76', '_c77', '_c78', '_c79']\n"
     ]
    }
   ],
   "source": [
    "# Show the names of all columns in a DataFrame\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1d6a122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-------------------+--------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+-------+-------------+-------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+------------------+------------------+------------------+------------------+-------------------+-------------+-------------+-------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------------------+-------------------+--------------------+-------------------+------------------+-------------------+--------------+--------------------+------------------+------------------+-----------------+------------------+--------------+--------------+----------------+--------------+--------------+----------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------+\n",
      "|summary|               _c0|              _c1|                _c2|                 _c3|               _c4|               _c5|               _c6|              _c7|               _c8|              _c9|             _c10|              _c11|              _c12|              _c13|              _c14|              _c15|   _c16|         _c17|               _c18|               _c19|               _c20|                _c21|                _c22|               _c23|               _c24|                _c25|                _c26|                _c27|              _c28|              _c29|              _c30|              _c31|               _c32|         _c33|         _c34|         _c35|              _c36|             _c37|             _c38|              _c39|              _c40|              _c41|              _c42|             _c43|              _c44|                _c45|               _c46|                _c47|               _c48|              _c49|               _c50|          _c51|                _c52|              _c53|              _c54|             _c55|              _c56|          _c57|          _c58|            _c59|          _c60|          _c61|            _c62|              _c63|              _c64|              _c65|             _c66|              _c67|              _c68|              _c69|              _c70|              _c71|              _c72|             _c73|              _c74|               _c75|                _c76|                _c77|                _c78|          _c79|\n",
      "+-------+------------------+-----------------+-------------------+--------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+-------+-------------+-------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+------------------+------------------+------------------+------------------+-------------------+-------------+-------------+-------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------------------+-------------------+--------------------+-------------------+------------------+-------------------+--------------+--------------------+------------------+------------------+-----------------+------------------+--------------+--------------+----------------+--------------+--------------+----------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------+\n",
      "|  count|           1048576|          1048576|            1048576|             1048576|           1048576|           1048576|           1048576|          1048576|           1048576|          1048576|          1048576|           1048576|           1048576|           1048576|           1048576|           1048576|1048576|      1048576|            1048576|            1048576|            1048576|             1048576|             1048576|            1048576|            1048576|             1048576|             1048576|             1048576|           1048576|           1048576|           1048576|           1048576|            1048576|      1048576|      1048576|      1048576|           1048576|          1048576|          1048576|           1048576|           1048576|           1048576|           1048576|          1048576|           1048576|             1048576|            1048576|             1048576|            1048576|           1048576|            1048576|       1048576|             1048576|           1048576|           1048576|          1048576|           1048576|       1048576|       1048576|         1048576|       1048576|       1048576|         1048576|           1048576|           1048576|           1048576|          1048576|           1048576|           1048576|           1048576|           1048576|           1048576|           1048576|          1048576|           1048576|            1048576|             1048576|             1048576|             1048576|       1048576|\n",
      "|   mean| 4876.261774312758|8.107557399327659|               null|   6255554.608775719| 6.206622320768663| 7.211191378775958|447.99364280094414|4521.803392222779|174.57359273299477|8.389535321746179|38.79578821097708|50.211073748340105| 332.5241427651813|20.165865102639295| 92.75989078818401|111.38990779107856|    NaN|     Infinity| 1583948.0319874296| 2101496.7788792206|  6306041.731401187| -1519512.7873628496|   5944302.856470925|  1775582.605887201| 2147776.1163238003|   6081985.071121284| -1475314.4025892282|   5754842.724734044|479811.02807397104| 604914.6569635285|1770278.8361671793| 88804.74067853992|0.02659418734949813|          0.0|          0.0|          0.0|156.00788117206685| 172.705021576902|141388.6265906406|125857.49684481233| 8.322709391316787|337.54171995326993| 63.11424061248737|95.70919239853886|30928.898699928228|0.003572467396228...|0.02659418734949813|0.039604224781250745| 0.5254311804115108|0.2634699473094438|0.12114631762153398|           0.0|0.039603271106024844|0.6634537348306034| 71.44444595531048|38.79578821097708|   92.759890788184|           0.0|           0.0|             0.0|           0.0|           0.0|             0.0| 6.206622320768663|447.99364280094414| 7.211191378775958|4521.803392222779|10501.662021314642|6142.2947113940345|2.7935359893188374|23.279704360679972|51524.487969143585| 21361.50958699804|87891.57214410033|39954.774385237106|  3101206.320220606|   729721.7626003549|   4812390.631669647|  2126919.6016422287|          null|\n",
      "| stddev|14443.436003519171| 4.46062464438205|               null|1.2602911086328785E9|44.478512313301124|104.86818042875132|15735.414790849307|151502.1211764956| 287.6713000641285|19.48278532244728|53.31882107631871| 85.18887318319754|493.85866850731173| 46.53823484510428|157.99966958037234|186.00815241243836|    NaN|          NaN|3.332476796130701E8|7.332937916481304E8|1.530125062737791E9|1.6408056604258738E9|1.2602869218031614E9|3.332505192542535E8|7.332951127120174E8|1.5301234294311178E9|1.6408061479790723E9|2.2579354992068004E7|2291241.8357854476|2612809.3380991886| 7985050.159868341|1690612.0557949296|0.16089425482742128|          0.0|          0.0|          0.0|1078.1193657900294|2103.964035310925|308526.4786857029| 267030.6296736084|18.906692623503766| 502.3828302829771|100.73397874849668|147.5420955204817|117069.74169216299| 0.05966329078869754|0.16089425482742128| 0.19502760428845728|0.49935307435233467|0.4405152883140702|0.32629739331791663|           0.0|  0.1950253529586814|0.5652833371543851|105.15198364228263|53.31882107631871|157.99966958037237|           0.0|           0.0|             0.0|           0.0|           0.0|             0.0|44.478512313301124|15735.414790849307|104.86818042875132|151502.1211764956|12937.092164614838|18149.800753580337| 5.557105952975703|11.061852084114566| 581558.6342225706|218640.49732394647|739572.4666968074| 560269.2675063498|5.414779701174505E8|3.8200308516858596E8|1.5221170424266853E9|1.8170133217088945E7|          null|\n",
      "|    min|                 0|                0|05/01/1970 03:01:17|        -11873000000|                 1|                 0|                 0|                0|                 0|                0|                0|                 0|                 0|                 0|                 0|                 0|      0|-0.0001684494|  -1051192460.31746|                  0|       -11873000000|        -11873000000|        -11873000000|  -1051192460.31746|                  0|        -11873000000|        -11873000000|                   0|                 0|                 0|                 0|                 0|                  0|            0|            0|            0|                 0|                0|                0|                 0|                 0|                 0|                 0|                0|                 0|                   0|                  0|                   0|                  0|                 0|                  0|             0|                   0|                 0|                 0|                0|                 0|             0|             0|               0|             0|             0|               0|                 1|                 0|                 0|                0|                -1|                -1|                 0|                 0|                 0|                 0|                0|                 0|                  0|                   0|                   0|                   0|        Benign|\n",
      "|    max|          Dst Port|         Protocol|          Timestamp|       Flow Duration|      Tot Fwd Pkts|      Tot Bwd Pkts|   TotLen Fwd Pkts|  TotLen Bwd Pkts|   Fwd Pkt Len Max|  Fwd Pkt Len Min| Fwd Pkt Len Mean|   Fwd Pkt Len Std|   Bwd Pkt Len Max|   Bwd Pkt Len Min|  Bwd Pkt Len Mean|   Bwd Pkt Len Std|    NaN|     Infinity|      Flow IAT Mean|       Flow IAT Std|       Flow IAT Max|        Flow IAT Min|         Fwd IAT Tot|       Fwd IAT Mean|        Fwd IAT Std|         Fwd IAT Max|         Fwd IAT Min|         Bwd IAT Tot|      Bwd IAT Mean|       Bwd IAT Std|       Bwd IAT Max|       Bwd IAT Min|      Fwd PSH Flags|Bwd PSH Flags|Fwd URG Flags|Bwd URG Flags|    Fwd Header Len|   Bwd Header Len|       Fwd Pkts/s|        Bwd Pkts/s|       Pkt Len Min|       Pkt Len Max|      Pkt Len Mean|      Pkt Len Std|       Pkt Len Var|        FIN Flag Cnt|       SYN Flag Cnt|        RST Flag Cnt|       PSH Flag Cnt|      ACK Flag Cnt|       URG Flag Cnt|CWE Flag Count|        ECE Flag Cnt|     Down/Up Ratio|      Pkt Size Avg| Fwd Seg Size Avg|  Bwd Seg Size Avg|Fwd Byts/b Avg|Fwd Pkts/b Avg|Fwd Blk Rate Avg|Bwd Byts/b Avg|Bwd Pkts/b Avg|Bwd Blk Rate Avg|  Subflow Fwd Pkts|  Subflow Fwd Byts|  Subflow Bwd Pkts| Subflow Bwd Byts| Init Fwd Win Byts| Init Bwd Win Byts| Fwd Act Data Pkts|  Fwd Seg Size Min|       Active Mean|        Active Std|       Active Max|        Active Min|          Idle Mean|            Idle Std|            Idle Max|            Idle Min|SSH-Bruteforce|\n",
      "+-------+------------------+-----------------+-------------------+--------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+-------+-------------+-------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+------------------+------------------+------------------+------------------+-------------------+-------------+-------------+-------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------------------+-------------------+--------------------+-------------------+------------------+-------------------+--------------+--------------------+------------------+------------------+-----------------+------------------+--------------+--------------+----------------+--------------+--------------+----------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary2 = df.describe()\n",
    "summary2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn('_c3_float', col('_c3').cast('float'))\n",
    "df = df.withColumn('_c0_float', col('_c0').cast('float'))\n",
    "\n",
    "corr_value = df.corr('_c0_float', '_c3_float')\n",
    "print(\"Correlation value between column1 and column4 is:\", corr_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c7ce9e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OneHotEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_366042/691715659.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Encode the categorical feature using OneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'indexed_c0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c0_encoded'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mencoded_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Assemble the feature columns into a single vector column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OneHotEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Encode the categorical feature using OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=['indexed_c0'], outputCols=['c0_encoded'])\n",
    "encoded_df = encoder.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "# Assemble the feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=['c0_encoded', '_c3'], outputCol='features')\n",
    "hypothesis_test_df = assembler.transform(encoded_df)\n",
    "\n",
    "# Create a ChiSquareTest object\n",
    "chisqTest = ChiSquareTest.test(hypothesis_test_df, 'features', '_c4')\n",
    "\n",
    "# Show the result of the hypothesis test\n",
    "chisqTest.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40e6427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_366042/2429560335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Generate range of values to evaluate density at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m x_vals = np.linspace(column.agg({\"Flow Duration\": \"min\"}).collect()[0][0], \n\u001b[0m\u001b[1;32m     23\u001b[0m                      \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Flow Duration\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"max\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                      num=1000)\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mlinspace\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/core/function_base.py\u001b[0m in \u001b[0;36mlinspace\u001b[0;34m(start, stop, num, endpoint, retstep, dtype, axis)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Convert float/complex array scalars to float, gh-3504\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# and make sure one can use variables that have an __array_interface__, gh-6634\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mstop\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m*\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32')"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data into PySpark DataFrame\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data.csv\")\n",
    "\n",
    "# Extract a single column as data to estimate density for\n",
    "column_name = \"Flow Duration\"\n",
    "column = data.select(col(column_name))\n",
    "\n",
    "# Define a user-defined function (UDF) to calculate Gaussian kernel density\n",
    "def gaussian_kernel_density(x, column, h):\n",
    "    kernel = np.exp(-(column - x)**2 / (2 * h**2)) / np.sqrt(2 * np.pi * h**2)\n",
    "    density = np.mean(kernel)\n",
    "    return density\n",
    "\n",
    "gaussian_kernel_density_udf = udf(lambda x: float(gaussian_kernel_density(x, column, h=0.1)), DoubleType())\n",
    "\n",
    "# Generate range of values to evaluate density at\n",
    "x_vals = np.linspace(column.agg({\"Flow Duration\": \"min\"}).collect()[0][0], \n",
    "                     column.agg({\"Flow Duration\": \"max\"}).collect()[0][0], \n",
    "                     num=1000)\n",
    "\n",
    "\n",
    "# Evaluate density at the generated values\n",
    "y_vals = np.array([gaussian_kernel_density_udf(x).item() for x in x_vals])\n",
    "\n",
    "# Plot density estimate\n",
    "plt.plot(x_vals, y_vals)\n",
    "plt.title('Density Estimate')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba30ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2523f136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load the data\n",
    "data = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Map the labels to 0 or 1 (attack or normal)\n",
    "data = data.withColumn(\"Label\", when(col(\"Label\") == \"BENIGN\", 0).otherwise(1))\n",
    "\n",
    "# Select only the required columns\n",
    "selected_cols = [\"Dst Port\", \"Protocol\", \"Tot Fwd Pkts\", \"Tot Bwd Pkts\", \"Pkt Len Mean\"]\n",
    "data = data.select(selected_cols + [\"Label\"])\n",
    "\n",
    "# Assemble the features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=selected_cols, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(training_data, testing_data) = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Train the decision tree classifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"Label\", featuresCol=\"features\", maxDepth=5)\n",
    "dt_model = dt.fit(training_data)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = dt_model.transform(testing_data)\n",
    "\n",
    "# Evaluate the model using the accuracy metric\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30fc19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "469752ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|Dst Port|FIN_Flag_Percentage|\n",
      "+--------+-------------------+\n",
      "|   57868|                0.0|\n",
      "|   35640|                0.0|\n",
      "|   51946|                0.0|\n",
      "|   37796|                0.0|\n",
      "|   43044|                0.0|\n",
      "|   34622|                0.0|\n",
      "|   57048|                0.0|\n",
      "|   54580|                0.0|\n",
      "|   52996|                0.0|\n",
      "|   37806|                0.0|\n",
      "|   40090|                0.0|\n",
      "|   35792|                0.0|\n",
      "|   46664|                0.0|\n",
      "|   60042|                0.0|\n",
      "|   37128|                0.0|\n",
      "|   44656|                0.0|\n",
      "|   49978|                0.0|\n",
      "|   50214|                0.0|\n",
      "|   53624|                0.0|\n",
      "|   56400|                0.0|\n",
      "+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TASK 1\n",
    "# Big Data Query & Analysis using Spark SQL [30 marks]\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession object\n",
    "spark = SparkSession.builder.appName(\"my_app\").getOrCreate()\n",
    "\n",
    "# load your data into a DataFrame\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(\"data.csv\")\n",
    "\n",
    "# create a temporary view of the DataFrame\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# run a SQL query against the view\n",
    "sql = \"SELECT `Dst Port`, (COUNT(CASE WHEN `FIN Flag Cnt` >= 2 THEN 1 END) / COUNT(*) * 100) AS FIN_Flag_Percentage FROM my_table GROUP BY `Dst Port` ORDER BY FIN_Flag_Percentage DESC\"\n",
    "result = spark.sql(sql)\n",
    "\n",
    "# display the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07f0422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Protocol|   Avg_Flow_Duration|\n",
      "+--------+--------------------+\n",
      "|       6|   9549803.667116841|\n",
      "|      17|  376613.15135208116|\n",
      "|       0|-1.21059038651405...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession object\n",
    "spark = SparkSession.builder.appName(\"my_app\").getOrCreate()\n",
    "\n",
    "# load your data into a DataFrame\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(\"data.csv\")\n",
    "\n",
    "# create a temporary view of the DataFrame\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# run a SQL query against the view\n",
    "sql = \"SELECT Protocol, AVG(`Flow Duration`) AS Avg_Flow_Duration FROM my_table GROUP BY Protocol ORDER BY Avg_Flow_Duration DESC LIMIT 5\"\n",
    "result = spark.sql(sql)\n",
    "\n",
    "# display the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cffb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
